{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3181bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.distributions import Normal as norm\n",
    "from termcolor import colored\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import linalg as LA\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multinomial\n",
    "from scipy.stats import logistic\n",
    "import random\n",
    "from Main_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e73b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(X_raw_T, X_raw_Test, Type = 'normalizing'):\n",
    "    \n",
    "    X_new_T = (X_raw_T - X_raw_T.mean(axis = 0))\n",
    "    X_new_Test = (X_raw_Test - X_raw_T.mean(axis = 0))\n",
    "    \n",
    "    if Type == 'normalizing':\n",
    "        std = X_new_T.std(axis = 0)\n",
    "        for i in range(len(std)):\n",
    "            if std[i] > 1e-3:\n",
    "                X_new_T[:, i] /= std[i]\n",
    "                X_new_Test[:, i] /= std[i]\n",
    "        \n",
    "    elif Type == 'center_scaling':\n",
    "        Max = X_new_T.max(axis = 0)\n",
    "        Min = X_new_T.min(axis = 0)\n",
    "        for i in range(len(Max)):\n",
    "            if Max[i] - Min[i] > 1e-3:\n",
    "                X_new_T[:, i]  = (X_new_T[:, i] - Min[i])/(Max[i] - Min[i])\n",
    "                X_new_Test[:, i]  = (X_new_Test[:, i] - Min[i])/(Max[i] - Min[i])\n",
    "            else: \n",
    "                X_new_T[:, i]  = (X_new_T[:, i] - Min[i])\n",
    "                X_new_Test[:, i]  = (X_new_Test[:, i] - Min[i])\n",
    "                \n",
    "    return X_new_T, X_new_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f024bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Title: Wisconsin Diagnostic Breast Cancer (WDBC)\n",
      "\n",
      "2. Source Information\n",
      "\n",
      "a) Creators: \n",
      "\n",
      "\tDr. William H. Wolberg, General Surgery Dept., University of\n",
      "\tWisconsin,  Clinical Sciences Center, Madison, WI 53792\n",
      "\twolberg@eagle.surgery.wisc.edu\n",
      "\n",
      "\tW. Nick Street, Computer Sciences Dept., University of\n",
      "\tWisconsin, 1210 West Dayton St., Madison, WI 53706\n",
      "\tstreet@cs.wisc.edu  608-262-6619\n",
      "\n",
      "\tOlvi L. Mangasarian, Computer Sciences Dept., University of\n",
      "\tWisconsin, 1210 West Dayton St., Madison, WI 53706\n",
      "\tolvi@cs.wisc.edu \n",
      "\n",
      "b) Donor: Nick Street\n",
      "\n",
      "c) Date: November 1995\n",
      "\n",
      "3. Past Usage:\n",
      "\n",
      "first usage:\n",
      "\n",
      "\tW.N. Street, W.H. Wolberg and O.L. Mangasarian \n",
      "\tNuclear feature extraction for breast tumor diagnosis.\n",
      "\tIS&T/SPIE 1993 International Symposium on Electronic Imaging: Science\n",
      "\tand Technology, volume 1905, pages 861-870, San Jose, CA, 1993.\n",
      "\n",
      "OR literature:\n",
      "\n",
      "\tO.L. Mangasarian, W.N. Street and W.H. Wolberg. \n",
      "\tBreast cancer diagnosis and prognosis via linear programming. \n",
      "\tOperations Research, 43(4), pages 570-577, July-August 1995.\n",
      "\n",
      "Medical literature:\n",
      "\n",
      "\tW.H. Wolberg, W.N. Street, and O.L. Mangasarian. \n",
      "\tMachine learning techniques to diagnose breast cancer from\n",
      "\tfine-needle aspirates.  \n",
      "\tCancer Letters 77 (1994) 163-171.\n",
      "\n",
      "\tW.H. Wolberg, W.N. Street, and O.L. Mangasarian. \n",
      "\tImage analysis and machine learning applied to breast cancer\n",
      "\tdiagnosis and prognosis.  \n",
      "\tAnalytical and Quantitative Cytology and Histology, Vol. 17\n",
      "\tNo. 2, pages 77-87, April 1995. \n",
      "\n",
      "\tW.H. Wolberg, W.N. Street, D.M. Heisey, and O.L. Mangasarian. \n",
      "\tComputerized breast cancer diagnosis and prognosis from fine\n",
      "\tneedle aspirates.  \n",
      "\tArchives of Surgery 1995;130:511-516.\n",
      "\n",
      "\tW.H. Wolberg, W.N. Street, D.M. Heisey, and O.L. Mangasarian. \n",
      "\tComputer-derived nuclear features distinguish malignant from\n",
      "\tbenign breast cytology.  \n",
      "\tHuman Pathology, 26:792--796, 1995.\n",
      "\n",
      "See also:\n",
      "\thttp://www.cs.wisc.edu/~olvi/uwmp/mpml.html\n",
      "\thttp://www.cs.wisc.edu/~olvi/uwmp/cancer.html\n",
      "\n",
      "Results:\n",
      "\n",
      "\t- predicting field 2, diagnosis: B = benign, M = malignant\n",
      "\t- sets are linearly separable using all 30 input features\n",
      "\t- best predictive accuracy obtained using one separating plane\n",
      "\t\tin the 3-D space of Worst Area, Worst Smoothness and\n",
      "\t\tMean Texture.  Estimated accuracy 97.5% using repeated\n",
      "\t\t10-fold crossvalidations.  Classifier has correctly\n",
      "\t\tdiagnosed 176 consecutive new patients as of November\n",
      "\t\t1995. \n",
      "\n",
      "4. Relevant information\n",
      "\n",
      "\tFeatures are computed from a digitized image of a fine needle\n",
      "\taspirate (FNA) of a breast mass.  They describe\n",
      "\tcharacteristics of the cell nuclei present in the image.\n",
      "\tA few of the images can be found at\n",
      "\thttp://www.cs.wisc.edu/~street/images/\n",
      "\n",
      "\tSeparating plane described above was obtained using\n",
      "\tMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "\tConstruction Via Linear Programming.\" Proceedings of the 4th\n",
      "\tMidwest Artificial Intelligence and Cognitive Science Society,\n",
      "\tpp. 97-101, 1992], a classification method which uses linear\n",
      "\tprogramming to construct a decision tree.  Relevant features\n",
      "\twere selected using an exhaustive search in the space of 1-4\n",
      "\tfeatures and 1-3 separating planes.\n",
      "\n",
      "\tThe actual linear program used to obtain the separating plane\n",
      "\tin the 3-dimensional space is that described in:\n",
      "\t[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "\tProgramming Discrimination of Two Linearly Inseparable Sets\",\n",
      "\tOptimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "\n",
      "\tThis database is also available through the UW CS ftp server:\n",
      "\n",
      "\tftp ftp.cs.wisc.edu\n",
      "\tcd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      "5. Number of instances: 569 \n",
      "\n",
      "6. Number of attributes: 32 (ID, diagnosis, 30 real-valued input features)\n",
      "\n",
      "7. Attribute information\n",
      "\n",
      "1) ID number\n",
      "2) Diagnosis (M = malignant, B = benign)\n",
      "3-32)\n",
      "\n",
      "Ten real-valued features are computed for each cell nucleus:\n",
      "\n",
      "\ta) radius (mean of distances from center to points on the perimeter)\n",
      "\tb) texture (standard deviation of gray-scale values)\n",
      "\tc) perimeter\n",
      "\td) area\n",
      "\te) smoothness (local variation in radius lengths)\n",
      "\tf) compactness (perimeter^2 / area - 1.0)\n",
      "\tg) concavity (severity of concave portions of the contour)\n",
      "\th) concave points (number of concave portions of the contour)\n",
      "\ti) symmetry \n",
      "\tj) fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "Several of the papers listed above contain detailed descriptions of\n",
      "how these features are computed. \n",
      "\n",
      "The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "largest values) of these features were computed for each image,\n",
      "resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
      "13 is Radius SE, field 23 is Worst Radius.\n",
      "\n",
      "All feature values are recoded with four significant digits.\n",
      "\n",
      "8. Missing attribute values: none\n",
      "\n",
      "9. Class distribution: 357 benign, 212 malignant\n"
     ]
    }
   ],
   "source": [
    "f = open(\"Data/breastcancer/wdbc.names\", \"r\")\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc290147",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Data/breastcancer/wdbc.data\", header = None)\n",
    "# test = pd.read_csv(\"Data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0143460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1      2      3       4       5        6        7       8   \\\n",
       "0    842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001   \n",
       "1    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
       "2  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
       "3  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
       "4  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
       "\n",
       "        9   ...     22     23      24      25      26      27      28      29  \\\n",
       "0  0.14710  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.07017  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.12790  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.10520  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.10430  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       30       31  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Number of records and columns\n",
    "print(train.shape)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79f19128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   1       569 non-null    object \n",
      " 1   2       569 non-null    float64\n",
      " 2   3       569 non-null    float64\n",
      " 3   4       569 non-null    float64\n",
      " 4   5       569 non-null    float64\n",
      " 5   6       569 non-null    float64\n",
      " 6   7       569 non-null    float64\n",
      " 7   8       569 non-null    float64\n",
      " 8   9       569 non-null    float64\n",
      " 9   10      569 non-null    float64\n",
      " 10  11      569 non-null    float64\n",
      " 11  12      569 non-null    float64\n",
      " 12  13      569 non-null    float64\n",
      " 13  14      569 non-null    float64\n",
      " 14  15      569 non-null    float64\n",
      " 15  16      569 non-null    float64\n",
      " 16  17      569 non-null    float64\n",
      " 17  18      569 non-null    float64\n",
      " 18  19      569 non-null    float64\n",
      " 19  20      569 non-null    float64\n",
      " 20  21      569 non-null    float64\n",
      " 21  22      569 non-null    float64\n",
      " 22  23      569 non-null    float64\n",
      " 23  24      569 non-null    float64\n",
      " 24  25      569 non-null    float64\n",
      " 25  26      569 non-null    float64\n",
      " 26  27      569 non-null    float64\n",
      " 27  28      569 non-null    float64\n",
      " 28  29      569 non-null    float64\n",
      " 29  30      569 non-null    float64\n",
      " 30  31      569 non-null    float64\n",
      "dtypes: float64(30), object(1)\n",
      "memory usage: 137.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "train = train.drop(0, axis=1)\n",
    "# train = train.drop('id', axis=1)\n",
    "# Check size of the data set\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af7ab01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  1      2      3       4       5        6        7       8        9       10  \\\n",
       "0  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "   ...     22     23      24      25      26      27      28      29      30  \\\n",
       "0  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654  0.4601   \n",
       "1  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860  0.2750   \n",
       "2  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430  0.3613   \n",
       "3  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575  0.6638   \n",
       "4  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625  0.2364   \n",
       "\n",
       "        31  \n",
       "0  0.11890  \n",
       "1  0.08902  \n",
       "2  0.08758  \n",
       "3  0.17300  \n",
       "4  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Number of records and columns\n",
    "print(train.shape)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d346076",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[1].replace({'M': 1, 'B': 0},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36a166ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (8,5))\n",
    "train[1].value_counts(normalize = True).plot(kind='bar', color= ['darkorange','steelblue'], alpha = 0.9, rot=0)\n",
    "plt.title('Indicator Benign = 0 and Malignant = 1 in the Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "977652c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f831616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "368992c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Type = 'center_scaling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e03f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type = 'normalizing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e31393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train.drop(columns=[1], inplace = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4180bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eb46271",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df713012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a267a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = labels[:500]\n",
    "D_test = labels[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5665790",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:500]\n",
    "X_test = X[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "265c385e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 30), (69, 30))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed, test_processed = pre_process(X_train, \n",
    "                                              X_test, \n",
    "                                              Type = Type) \n",
    "train_processed.shape, test_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e682a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d = train_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c98e8c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500,), (69,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_train.shape, D_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e4c7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_T = torch.tensor(train_processed, dtype = torch.float64)\n",
    "Y_T = torch.tensor(np.zeros(train_processed.shape), dtype = torch.float64)\n",
    "D_T = torch.tensor(D_train, dtype = torch.torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ec6f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor(test_processed, dtype = torch.float64)\n",
    "Y_test = torch.tensor(np.zeros(test_processed.shape), dtype = torch.float64)\n",
    "D_test = torch.tensor(D_test, dtype = torch.torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ade7d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([69, 30]), torch.Size([500, 30]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, X_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0568412",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = X_T.shape[1] \n",
    "k = d\n",
    "n_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee7262ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ML(d, k, n_labels, \n",
    "           X_T, Y_T,  D_T, D_T, \n",
    "           X_test, Y_test, D_test, D_test, \n",
    "           Type = 'Laplace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33b2c858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Tau:  tensor([0.4047], dtype=torch.float64, requires_grad=True)\n",
      "epoch 0: loss = 0.6757308682359591\n",
      "train accuracy with noise 0.61\n",
      "train accuracy without noise 0.61\n",
      "test accuracy with noise 0.7536231884057971\n",
      "test accuracy without noise 0.7536231884057971\n",
      "epoch 5000:\n",
      " norm of B.grad = 6.562566794404207e-05,\n",
      " loss = 0.07622272438623767\n",
      "tensor([4.7831], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.984\n",
      "train accuracy without noise 0.984\n",
      "test accuracy with noise 0.9855072463768116\n",
      "test accuracy without noise 0.9855072463768116\n",
      "epoch 10000:\n",
      " norm of B.grad = 3.230506256410635e-05,\n",
      " loss = 0.06193007205902619\n",
      "tensor([5.8368], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.984\n",
      "train accuracy without noise 0.984\n",
      "test accuracy with noise 0.9855072463768116\n",
      "test accuracy without noise 0.9855072463768116\n",
      "epoch 15000:\n",
      " norm of B.grad = 1.4241255084227881e-05,\n",
      " loss = 0.055638374833636615\n",
      "tensor([6.4857], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.986\n",
      "train accuracy without noise 0.986\n",
      "test accuracy with noise 0.9855072463768116\n",
      "test accuracy without noise 0.9855072463768116\n",
      "epoch 20000:\n",
      " norm of B.grad = 8.846534078285352e-06,\n",
      " loss = 0.05248948250865928\n",
      "tensor([6.9555], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.986\n",
      "train accuracy without noise 0.986\n",
      "test accuracy with noise 0.9855072463768116\n",
      "test accuracy without noise 0.9855072463768116\n"
     ]
    }
   ],
   "source": [
    "model.train(learning_rate = 3e-2, #3.5e-2 --> 98%\n",
    "            n_iters = 20001, \n",
    "            decay = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c830af67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = (D_test == 1) # 'M': 1, 'B': 0\n",
    "I.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b175e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_M = X_test[I]\n",
    "Y_test_M = Y_test[I]\n",
    "D_test_M = D_test[I]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f899daa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.accuracy(X_test_M, Y_test_M, D_test_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8a7bd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(52)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J = (D_test == 0)\n",
    "J.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc42c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_B = X_test[J]\n",
    "Y_test_B = Y_test[J]\n",
    "D_test_B = D_test[J]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84c90af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9807692307692307"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.accuracy(X_test_B, Y_test_B, D_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c0a1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_T, Y_T,  D_T,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e268d5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(195)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "II = (D_T == 1) # 'M': 1, 'B': 0\n",
    "II.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35e00218",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_T_M = X_T[II]\n",
    "Y_T_M = Y_T[II]\n",
    "D_T_M = D_T[II]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59bf4fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9692307692307692"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.accuracy(X_T_M, Y_T_M, D_T_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6bbe67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9127c91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(305)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JJ = (D_T == 0) # 'M': 1, 'B': 0\n",
    "JJ.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8b96b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_T_B = X_T[JJ]\n",
    "Y_T_B = Y_T[JJ]\n",
    "D_T_B = D_T[JJ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2af188ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9967213114754099"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.accuracy(X_T_B, Y_T_B, D_T_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58103cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5adc08d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = (model.B @ model.B.T).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1cb08243",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, eigs, _ = LA.svd(M, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e42e9e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.16323390e+01, 4.84510191e-09, 1.60295465e-10, 1.21488424e-10,\n",
       "       8.98846694e-11, 6.88836519e-11, 6.00472077e-11, 4.94499951e-11,\n",
       "       4.77041637e-11, 4.35888184e-11, 3.87857189e-11, 3.52636463e-11,\n",
       "       2.93353567e-11, 2.74390412e-11, 2.60636059e-11, 1.86738994e-11,\n",
       "       1.68680267e-11, 1.15491790e-11, 9.63048770e-12, 7.64471967e-12,\n",
       "       5.97638193e-12, 4.28018476e-12, 2.85136306e-12, 2.40857824e-12,\n",
       "       1.74826655e-12, 7.56140898e-13, 2.86514050e-13, 6.43857042e-14,\n",
       "       6.30596279e-15, 2.15760864e-15])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5b8c49",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f11f1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train.drop(columns=[1], inplace = False))\n",
    "labels = np.array(train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "578b7b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Tau:  tensor([0.5197], dtype=torch.float64, requires_grad=True)\n",
      "epoch 0: loss = 0.6739452641888318\n",
      "train accuracy with noise 0.6266666666666667\n",
      "train accuracy without noise 0.6266666666666667\n",
      "test accuracy with noise 0.6302521008403361\n",
      "test accuracy without noise 0.6302521008403361\n",
      "epoch 5000:\n",
      " norm of B.grad = 5.005079646919177e-05,\n",
      " loss = 0.05965927060288434\n",
      "tensor([5.0223], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9866666666666667\n",
      "train accuracy without noise 0.9866666666666667\n",
      "test accuracy with noise 0.9747899159663865\n",
      "test accuracy without noise 0.9747899159663865\n",
      "epoch 10000:\n",
      " norm of B.grad = 1.9348954226290288e-05,\n",
      " loss = 0.04695896932266451\n",
      "tensor([6.1295], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9911111111111112\n",
      "train accuracy without noise 0.9911111111111112\n",
      "test accuracy with noise 0.9747899159663865\n",
      "test accuracy without noise 0.9747899159663865\n",
      "epoch 15000:\n",
      " norm of B.grad = 1.1381915459999138e-05,\n",
      " loss = 0.04167674282143202\n",
      "tensor([6.8098], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9911111111111112\n",
      "train accuracy without noise 0.9911111111111112\n",
      "test accuracy with noise 0.9747899159663865\n",
      "test accuracy without noise 0.9747899159663865\n",
      "epoch 20000:\n",
      " norm of B.grad = 8.075072972838003e-06,\n",
      " loss = 0.03862263909488459\n",
      "tensor([7.3021], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9911111111111112\n",
      "train accuracy without noise 0.9911111111111112\n",
      "test accuracy with noise 0.9747899159663865\n",
      "test accuracy without noise 0.9747899159663865\n",
      "Starting Tau:  tensor([0.2970], dtype=torch.float64, requires_grad=True)\n",
      "epoch 0: loss = 0.6572083876981963\n",
      "train accuracy with noise 0.6333333333333333\n",
      "train accuracy without noise 0.6333333333333333\n",
      "test accuracy with noise 0.6050420168067226\n",
      "test accuracy without noise 0.6050420168067226\n",
      "epoch 5000:\n",
      " norm of B.grad = 5.7014530036185176e-05,\n",
      " loss = 0.06845292440388162\n",
      "tensor([4.9047], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.98\n",
      "train accuracy without noise 0.98\n",
      "test accuracy with noise 0.9831932773109243\n",
      "test accuracy without noise 0.9831932773109243\n",
      "epoch 10000:\n",
      " norm of B.grad = 2.9490266283182242e-05,\n",
      " loss = 0.055178372726339214\n",
      "tensor([5.9491], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9866666666666667\n",
      "train accuracy without noise 0.9866666666666667\n",
      "test accuracy with noise 0.9831932773109243\n",
      "test accuracy without noise 0.9831932773109243\n",
      "epoch 15000:\n",
      " norm of B.grad = 2.05947169447607e-05,\n",
      " loss = 0.048230500103643395\n",
      "tensor([6.6123], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9888888888888889\n",
      "train accuracy without noise 0.9888888888888889\n",
      "test accuracy with noise 0.9831932773109243\n",
      "test accuracy without noise 0.9831932773109243\n",
      "epoch 20000:\n",
      " norm of B.grad = 1.0540765821943503e-05,\n",
      " loss = 0.04442089736506427\n",
      "tensor([7.1201], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9911111111111112\n",
      "train accuracy without noise 0.9911111111111112\n",
      "test accuracy with noise 0.9831932773109243\n",
      "test accuracy without noise 0.9831932773109243\n",
      "Starting Tau:  tensor([0.6774], dtype=torch.float64, requires_grad=True)\n",
      "epoch 0: loss = 0.6856904081257934\n",
      "train accuracy with noise 0.6355555555555555\n",
      "train accuracy without noise 0.6355555555555555\n",
      "test accuracy with noise 0.5966386554621849\n",
      "test accuracy without noise 0.5966386554621849\n",
      "epoch 5000:\n",
      " norm of B.grad = 6.382476443627476e-05,\n",
      " loss = 0.069135313786238\n",
      "tensor([4.9765], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9866666666666667\n",
      "train accuracy without noise 0.9866666666666667\n",
      "test accuracy with noise 0.9831932773109243\n",
      "test accuracy without noise 0.9831932773109243\n",
      "epoch 10000:\n",
      " norm of B.grad = 2.6445421802877553e-05,\n",
      " loss = 0.05383760123255093\n",
      "tensor([6.0920], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9888888888888889\n",
      "train accuracy without noise 0.9888888888888889\n",
      "test accuracy with noise 0.9747899159663865\n",
      "test accuracy without noise 0.9747899159663865\n",
      "epoch 15000:\n",
      " norm of B.grad = 1.284358472060627e-05,\n",
      " loss = 0.04785239381128134\n",
      "tensor([6.7951], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9888888888888889\n",
      "train accuracy without noise 0.9888888888888889\n",
      "test accuracy with noise 0.9747899159663865\n",
      "test accuracy without noise 0.9747899159663865\n",
      "epoch 20000:\n",
      " norm of B.grad = 8.457409279707702e-06,\n",
      " loss = 0.04455969172684319\n",
      "tensor([7.3023], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9933333333333333\n",
      "train accuracy without noise 0.9933333333333333\n",
      "test accuracy with noise 0.9747899159663865\n",
      "test accuracy without noise 0.9747899159663865\n",
      "Starting Tau:  tensor([0.1326], dtype=torch.float64, requires_grad=True)\n",
      "epoch 0: loss = 0.6742915690807285\n",
      "train accuracy with noise 0.6066666666666667\n",
      "train accuracy without noise 0.6066666666666667\n",
      "test accuracy with noise 0.7058823529411765\n",
      "test accuracy without noise 0.7058823529411765\n",
      "epoch 5000:\n",
      " norm of B.grad = 6.101877390663697e-05,\n",
      " loss = 0.07637510360809675\n",
      "tensor([4.7933], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9822222222222222\n",
      "train accuracy without noise 0.9822222222222222\n",
      "test accuracy with noise 0.9831932773109243\n",
      "test accuracy without noise 0.9831932773109243\n",
      "epoch 10000:\n",
      " norm of B.grad = 3.631164587231227e-05,\n",
      " loss = 0.06304350170073886\n",
      "tensor([5.7753], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9822222222222222\n",
      "train accuracy without noise 0.9822222222222222\n",
      "test accuracy with noise 0.9831932773109243\n",
      "test accuracy without noise 0.9831932773109243\n",
      "epoch 15000:\n",
      " norm of B.grad = 1.3564367325064382e-05,\n",
      " loss = 0.057515212729925415\n",
      "tensor([6.3768], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9844444444444445\n",
      "train accuracy without noise 0.9844444444444445\n",
      "test accuracy with noise 0.9915966386554622\n",
      "test accuracy without noise 0.9915966386554622\n",
      "epoch 20000:\n",
      " norm of B.grad = 9.649763984501593e-06,\n",
      " loss = 0.05464114420532111\n",
      "tensor([6.7958], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9866666666666667\n",
      "train accuracy without noise 0.9866666666666667\n",
      "test accuracy with noise 0.9915966386554622\n",
      "test accuracy without noise 0.9915966386554622\n",
      "Starting Tau:  tensor([0.1599], dtype=torch.float64, requires_grad=True)\n",
      "epoch 0: loss = 0.6711056987641564\n",
      "train accuracy with noise 0.6111111111111112\n",
      "train accuracy without noise 0.6111111111111112\n",
      "test accuracy with noise 0.6890756302521008\n",
      "test accuracy without noise 0.6890756302521008\n",
      "epoch 5000:\n",
      " norm of B.grad = 5.3430754713235444e-05,\n",
      " loss = 0.07205631036418005\n",
      "tensor([4.9267], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9866666666666667\n",
      "train accuracy without noise 0.9866666666666667\n",
      "test accuracy with noise 0.9831932773109243\n",
      "test accuracy without noise 0.9831932773109243\n",
      "epoch 10000:\n",
      " norm of B.grad = 2.2509509967466514e-05,\n",
      " loss = 0.06033272529463772\n",
      "tensor([5.9137], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9888888888888889\n",
      "train accuracy without noise 0.9888888888888889\n",
      "test accuracy with noise 0.9831932773109243\n",
      "test accuracy without noise 0.9831932773109243\n",
      "epoch 15000:\n",
      " norm of B.grad = 1.409045010979646e-05,\n",
      " loss = 0.055441208814581235\n",
      "tensor([6.4865], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9888888888888889\n",
      "train accuracy without noise 0.9888888888888889\n",
      "test accuracy with noise 0.9831932773109243\n",
      "test accuracy without noise 0.9831932773109243\n",
      "epoch 20000:\n",
      " norm of B.grad = 1.0087261656428117e-05,\n",
      " loss = 0.052565992975506806\n",
      "tensor([6.8889], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9911111111111112\n",
      "train accuracy without noise 0.9911111111111112\n",
      "test accuracy with noise 0.9747899159663865\n",
      "test accuracy without noise 0.9747899159663865\n",
      "Starting Tau:  tensor([0.2486], dtype=torch.float64, requires_grad=True)\n",
      "epoch 0: loss = 0.6602950115345073\n",
      "train accuracy with noise 0.6288888888888889\n",
      "train accuracy without noise 0.6288888888888889\n",
      "test accuracy with noise 0.6218487394957983\n",
      "test accuracy without noise 0.6218487394957983\n",
      "epoch 5000:\n",
      " norm of B.grad = 7.373655654278663e-05,\n",
      " loss = 0.06833056582668617\n",
      "tensor([4.9092], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9866666666666667\n",
      "train accuracy without noise 0.9866666666666667\n",
      "test accuracy with noise 0.9663865546218487\n",
      "test accuracy without noise 0.9663865546218487\n",
      "epoch 10000:\n",
      " norm of B.grad = 2.5286876127761728e-05,\n",
      " loss = 0.05332886567281938\n",
      "tensor([5.9864], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9933333333333333\n",
      "train accuracy without noise 0.9933333333333333\n",
      "test accuracy with noise 0.9663865546218487\n",
      "test accuracy without noise 0.9663865546218487\n",
      "epoch 15000:\n",
      " norm of B.grad = 1.3660107735058503e-05,\n",
      " loss = 0.047591288632380276\n",
      "tensor([6.6527], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9933333333333333\n",
      "train accuracy without noise 0.9933333333333333\n",
      "test accuracy with noise 0.9663865546218487\n",
      "test accuracy without noise 0.9663865546218487\n",
      "epoch 20000:\n",
      " norm of B.grad = 9.367976611378944e-06,\n",
      " loss = 0.04434595618113802\n",
      "tensor([7.1350], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9933333333333333\n",
      "train accuracy without noise 0.9933333333333333\n",
      "test accuracy with noise 0.957983193277311\n",
      "test accuracy without noise 0.957983193277311\n",
      "Starting Tau:  tensor([0.7472], dtype=torch.float64, requires_grad=True)\n",
      "epoch 0: loss = 0.6993121975589643\n",
      "train accuracy with noise 0.6333333333333333\n",
      "train accuracy without noise 0.6333333333333333\n",
      "test accuracy with noise 0.6050420168067226\n",
      "test accuracy without noise 0.6050420168067226\n",
      "epoch 5000:\n",
      " norm of B.grad = 6.257669126565654e-05,\n",
      " loss = 0.062289967917848986\n",
      "tensor([5.0583], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9866666666666667\n",
      "train accuracy without noise 0.9866666666666667\n",
      "test accuracy with noise 0.957983193277311\n",
      "test accuracy without noise 0.957983193277311\n",
      "epoch 10000:\n",
      " norm of B.grad = 3.642989723222642e-05,\n",
      " loss = 0.04708635699540202\n",
      "tensor([6.1731], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9888888888888889\n",
      "train accuracy without noise 0.9888888888888889\n",
      "test accuracy with noise 0.957983193277311\n",
      "test accuracy without noise 0.957983193277311\n",
      "epoch 15000:\n",
      " norm of B.grad = 1.4593438666221048e-05,\n",
      " loss = 0.04034794193094795\n",
      "tensor([6.8856], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9911111111111112\n",
      "train accuracy without noise 0.9911111111111112\n",
      "test accuracy with noise 0.957983193277311\n",
      "test accuracy without noise 0.957983193277311\n",
      "epoch 20000:\n",
      " norm of B.grad = 1.0177775040641011e-05,\n",
      " loss = 0.03670034088518589\n",
      "tensor([7.4100], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9911111111111112\n",
      "train accuracy without noise 0.9911111111111112\n",
      "test accuracy with noise 0.957983193277311\n",
      "test accuracy without noise 0.957983193277311\n",
      "Starting Tau:  tensor([0.0545], dtype=torch.float64, requires_grad=True)\n",
      "epoch 0: loss = 0.6801716255853664\n",
      "train accuracy with noise 0.6355555555555555\n",
      "train accuracy without noise 0.6355555555555555\n",
      "test accuracy with noise 0.5966386554621849\n",
      "test accuracy without noise 0.5966386554621849\n",
      "epoch 5000:\n",
      " norm of B.grad = 5.919174935003387e-05,\n",
      " loss = 0.07135498272875056\n",
      "tensor([4.9743], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9844444444444445\n",
      "train accuracy without noise 0.9844444444444445\n",
      "test accuracy with noise 0.9831932773109243\n",
      "test accuracy without noise 0.9831932773109243\n",
      "epoch 10000:\n",
      " norm of B.grad = 3.262980783629486e-05,\n",
      " loss = 0.05723611697832018\n",
      "tensor([6.0229], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9866666666666667\n",
      "train accuracy without noise 0.9866666666666667\n",
      "test accuracy with noise 0.9831932773109243\n",
      "test accuracy without noise 0.9831932773109243\n",
      "epoch 15000:\n",
      " norm of B.grad = 1.3274977679647573e-05,\n",
      " loss = 0.05144641301061597\n",
      "tensor([6.6814], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9888888888888889\n",
      "train accuracy without noise 0.9888888888888889\n",
      "test accuracy with noise 0.9831932773109243\n",
      "test accuracy without noise 0.9831932773109243\n",
      "epoch 20000:\n",
      " norm of B.grad = 9.417038463664814e-06,\n",
      " loss = 0.048328663787684516\n",
      "tensor([7.1480], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9888888888888889\n",
      "train accuracy without noise 0.9888888888888889\n",
      "test accuracy with noise 0.9831932773109243\n",
      "test accuracy without noise 0.9831932773109243\n",
      "Starting Tau:  tensor([0.9656], dtype=torch.float64, requires_grad=True)\n",
      "epoch 0: loss = 0.7644872281944208\n",
      "train accuracy with noise 0.6177777777777778\n",
      "train accuracy without noise 0.6177777777777778\n",
      "test accuracy with noise 0.6638655462184874\n",
      "test accuracy without noise 0.6638655462184874\n",
      "epoch 5000:\n",
      " norm of B.grad = 6.198664824387664e-05,\n",
      " loss = 0.0703945168889764\n",
      "tensor([4.9958], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9844444444444445\n",
      "train accuracy without noise 0.9844444444444445\n",
      "test accuracy with noise 0.9747899159663865\n",
      "test accuracy without noise 0.9747899159663865\n",
      "epoch 10000:\n",
      " norm of B.grad = 2.2622684141124663e-05,\n",
      " loss = 0.05639544214827693\n",
      "tensor([6.0687], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9888888888888889\n",
      "train accuracy without noise 0.9888888888888889\n",
      "test accuracy with noise 0.9831932773109243\n",
      "test accuracy without noise 0.9831932773109243\n",
      "epoch 15000:\n",
      " norm of B.grad = 1.2265754323074688e-05,\n",
      " loss = 0.051114373906941686\n",
      "tensor([6.7148], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9911111111111112\n",
      "train accuracy without noise 0.9911111111111112\n",
      "test accuracy with noise 0.9831932773109243\n",
      "test accuracy without noise 0.9831932773109243\n",
      "epoch 20000:\n",
      " norm of B.grad = 8.351660934322031e-06,\n",
      " loss = 0.04821608849433825\n",
      "tensor([7.1699], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9911111111111112\n",
      "train accuracy without noise 0.9911111111111112\n",
      "test accuracy with noise 0.9831932773109243\n",
      "test accuracy without noise 0.9831932773109243\n",
      "Starting Tau:  tensor([0.3314], dtype=torch.float64, requires_grad=True)\n",
      "epoch 0: loss = 0.6585753664019903\n",
      "train accuracy with noise 0.6311111111111111\n",
      "train accuracy without noise 0.6311111111111111\n",
      "test accuracy with noise 0.6134453781512605\n",
      "test accuracy without noise 0.6134453781512605\n",
      "epoch 5000:\n",
      " norm of B.grad = 8.382298516645782e-05,\n",
      " loss = 0.06945531853281711\n",
      "tensor([4.8207], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9844444444444445\n",
      "train accuracy without noise 0.9844444444444445\n",
      "test accuracy with noise 0.9663865546218487\n",
      "test accuracy without noise 0.9663865546218487\n",
      "epoch 10000:\n",
      " norm of B.grad = 2.4574425878816315e-05,\n",
      " loss = 0.055737484523677897\n",
      "tensor([5.8313], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9888888888888889\n",
      "train accuracy without noise 0.9888888888888889\n",
      "test accuracy with noise 0.9663865546218487\n",
      "test accuracy without noise 0.9663865546218487\n",
      "epoch 15000:\n",
      " norm of B.grad = 1.4315092372917291e-05,\n",
      " loss = 0.05056776819878514\n",
      "tensor([6.4223], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9911111111111112\n",
      "train accuracy without noise 0.9911111111111112\n",
      "test accuracy with noise 0.9663865546218487\n",
      "test accuracy without noise 0.9663865546218487\n",
      "epoch 20000:\n",
      " norm of B.grad = 9.978088397919803e-06,\n",
      " loss = 0.04762100796854808\n",
      "tensor([6.8388], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9911111111111112\n",
      "train accuracy without noise 0.9911111111111112\n",
      "test accuracy with noise 0.9663865546218487\n",
      "test accuracy without noise 0.9663865546218487\n"
     ]
    }
   ],
   "source": [
    "N_I = N_J = N_II = N_JJ = 0\n",
    "\n",
    "Model_history = []\n",
    "\n",
    "Train_accuracy = []\n",
    "Test_accuracy = [] \n",
    "\n",
    "B_Train_accuracy = []\n",
    "M_Train_accuracy = []\n",
    "\n",
    "B_Test_accuracy = []\n",
    "M_Test_accuracy = []\n",
    "\n",
    "n = len(labels)\n",
    "\n",
    "for _ in range(10):\n",
    "    I = list(range(n))\n",
    "    np.random.shuffle(I)\n",
    "    X_train = X[I][:450]\n",
    "    X_test = X[I][450:]\n",
    "    D_train = labels[I][:450]\n",
    "    D_test = labels[I][450:]\n",
    "    train_processed, test_processed = pre_process(X_train, \n",
    "                                                  X_test, \n",
    "                                                  Type = Type) \n",
    "    train_processed.shape, test_processed.shape\n",
    "    X_T = torch.tensor(train_processed, dtype = torch.float64)\n",
    "    Y_T = torch.tensor(np.zeros(train_processed.shape), dtype = torch.float64)\n",
    "    D_T = torch.tensor(D_train, dtype = torch.torch.int64)\n",
    "    \n",
    "    X_test = torch.tensor(test_processed, dtype = torch.float64)\n",
    "    Y_test = torch.tensor(np.zeros(test_processed.shape), dtype = torch.float64)\n",
    "    D_test = torch.tensor(D_test, dtype = torch.torch.int64)\n",
    "    model = ML(d, k, n_labels, \n",
    "               X_T, Y_T,  D_T, D_T, \n",
    "               X_test, Y_test, D_test, D_test, \n",
    "               Type = 'Laplace')\n",
    "    model.train(learning_rate = 3.5e-2, #3.5e-2 --> 98%\n",
    "                n_iters = 20001, \n",
    "                decay = 0.95)\n",
    "    \n",
    "    Model_history.append(model)\n",
    "    \n",
    "    Train_accuracy.append(model.accuracy(X_T, Y_T, D_T))\n",
    "    Test_accuracy.append(model.accuracy(X_test, Y_test, D_test)) \n",
    "    \n",
    "    \n",
    "    \n",
    "    I = (D_T == 1) # M = 1 \n",
    "    N_I += sum(I)\n",
    "    \n",
    "    J = (D_T == 0) # B = 0 \n",
    "    N_J += sum(J)\n",
    "    \n",
    "    X_T_M = X_T[I]\n",
    "    Y_T_M = Y_T[I]\n",
    "    D_T_M = D_T[I]\n",
    "    \n",
    "    X_T_B = X_T[J]\n",
    "    Y_T_B = Y_T[J]\n",
    "    D_T_B = D_T[J]\n",
    "\n",
    "    \n",
    "    M_Train_accuracy.append((I.sum(), model.accuracy(X_T_M, Y_T_M, D_T_M))) \n",
    "    B_Train_accuracy.append((J.sum(), model.accuracy(X_T_B, Y_T_B, D_T_B)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    II = (D_test == 1) # M = 1 \n",
    "    N_II += sum(II)\n",
    "    \n",
    "    JJ = (D_test == 0) # B = 0 \n",
    "    N_JJ += sum(JJ)\n",
    "    \n",
    "    X_test_M = X_test[II]\n",
    "    Y_test_M = Y_test[II]\n",
    "    D_test_M = D_test[II]\n",
    "    \n",
    "    X_test_B = X_test[JJ]\n",
    "    Y_test_B = Y_test[JJ]\n",
    "    D_test_B = D_test[JJ]\n",
    "    \n",
    "    M_Test_accuracy.append((II.sum(), model.accuracy(X_test_M, Y_test_M, D_test_M)))\n",
    "    B_Test_accuracy.append((JJ.sum(), model.accuracy(X_test_B, Y_test_B, D_test_B)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56ac22cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = sum(Train_accuracy)/10\n",
    "test_accuracy = sum(Test_accuracy)/10\n",
    "\n",
    "b_train_accuracy = 0\n",
    "b_test_accuracy = 0\n",
    "\n",
    "m_train_accuracy = 0\n",
    "m_test_accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee61869e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.990888888888889, 0.9747899159663864)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuracy, test_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27c02ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    len_, acc_ = M_Train_accuracy[i]\n",
    "    m_train_accuracy += (len_*acc_/N_I) \n",
    "    \n",
    "    len_, acc_ = B_Train_accuracy[i]\n",
    "    b_train_accuracy += (len_*acc_/N_J)\n",
    "    \n",
    "    len_, acc_ = M_Test_accuracy[i]\n",
    "    m_test_accuracy += (len_*acc_/N_II) \n",
    "    \n",
    "    len_, acc_ = B_Test_accuracy[i]\n",
    "    b_test_accuracy += (len_*acc_/N_JJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db9fa572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9792), tensor(0.9979), tensor(0.9588), tensor(0.9841))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_train_accuracy, b_train_accuracy, m_test_accuracy, b_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7cca44b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.54284715e-01, 1.17895281e-01, 4.73600170e-02, 4.22168486e-02,\n",
       "       2.86745637e-02, 1.95555134e-02, 1.00000405e-02, 8.82168012e-03,\n",
       "       6.76907744e-03, 6.30084735e-03, 5.51844358e-03, 4.50675880e-03,\n",
       "       4.30119605e-03, 3.60671210e-03, 1.75541012e-03, 1.60760990e-03,\n",
       "       1.06248851e-03, 8.80933691e-04, 8.37090769e-04, 6.32526126e-04,\n",
       "       5.95614080e-04, 5.09272884e-04, 3.36736857e-04, 3.07137230e-04,\n",
       "       2.98698158e-04, 2.30465116e-04, 1.26245260e-04, 3.73081661e-05,\n",
       "       1.62411600e-05, 3.99942100e-06])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.cov(train_processed.T)\n",
    "U_c, D_C, V_c = LA.svd(C, full_matrices = False)\n",
    "D_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae5705d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_C_inv = U_c @ np.diag(np.sqrt(D_C)**-1) @ V_c\n",
    "sqrt_C = U_c @ np.diag(np.sqrt(D_C)) @ V_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e6c7e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed_normal = train_processed @ sqrt_C_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "68e1a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed_normal = test_processed @ sqrt_C_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "220d6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_T = np.array(labels[:450])\n",
    "D_test = np.array(labels[450:])\n",
    "\n",
    "X_T = torch.tensor(train_processed, dtype = torch.float64)\n",
    "Y_T = torch.tensor(np.zeros(train_processed.shape), dtype = torch.float64)\n",
    "D_T = torch.tensor(D_train, dtype = torch.torch.int64)\n",
    "\n",
    "X_test = torch.tensor(test_processed, dtype = torch.float64)\n",
    "Y_test = torch.tensor(np.zeros(test_processed.shape), dtype = torch.float64)\n",
    "D_test = torch.tensor(D_test, dtype = torch.torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed51a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_normal = ML(d, k, n_labels, \n",
    "           X_T, Y_T,  D_T, D_T, \n",
    "           X_test, Y_test, D_test, D_test, Type = 'Logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f8a2dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Tau:  tensor([0.2214], dtype=torch.float64, requires_grad=True)\n",
      "epoch 0: loss = 0.6702332951985358\n",
      "train accuracy with noise 0.6311111111111111\n",
      "train accuracy without noise 0.6311111111111111\n",
      "test accuracy with noise 0.773109243697479\n",
      "test accuracy without noise 0.773109243697479\n",
      "epoch 5000:\n",
      " norm of B.grad = 0.011075210523326029,\n",
      " loss = 0.6544306689769225\n",
      "tensor([0.4847], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.6311111111111111\n",
      "train accuracy without noise 0.6311111111111111\n",
      "test accuracy with noise 0.773109243697479\n",
      "test accuracy without noise 0.773109243697479\n",
      "epoch 10000:\n",
      " norm of B.grad = 0.007243617607066889,\n",
      " loss = 0.3433915973511718\n",
      "tensor([1.3164], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9466666666666667\n",
      "train accuracy without noise 0.9466666666666667\n",
      "test accuracy with noise 0.5630252100840336\n",
      "test accuracy without noise 0.5630252100840336\n",
      "epoch 15000:\n",
      " norm of B.grad = 0.002615294868978116,\n",
      " loss = 0.249279002269421\n",
      "tensor([1.9732], dtype=torch.float64, requires_grad=True)\n",
      "train accuracy with noise 0.9555555555555556\n",
      "train accuracy without noise 0.9555555555555556\n",
      "test accuracy with noise 0.5630252100840336\n",
      "test accuracy without noise 0.5630252100840336\n"
     ]
    }
   ],
   "source": [
    "model_normal.train(learning_rate = 1.5e-3, \n",
    "            n_iters = 15001, \n",
    "            decay = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b54d87d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
